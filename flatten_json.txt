def flattenIterativeV2(dfGlobal: DataFrame): DataFrame = {
		var df: DataFrame = dfGlobal
		var flag = true //allow first loop
		 while(flag){
			flag = false //reset every loop
			df.schema.fields.foreach {
				elem =>
				var fieldNames = df.schema.fields.map(x => x.name)
				elem.dataType match {
				case arrayType: ArrayType => //println("flatten array")
					flag = true
					fieldNames = fieldNames.filter(_!=elem.name) ++ Array("explode_outer(".concat(elem.name).concat(") as ").concat(elem.name))
					df=df.selectExpr(fieldNames:_*)
				case structType: StructType => //println("flatten struct")
					flag = true
					fieldNames = fieldNames.filter(_!=elem.name) ++ 
          										structType.fieldNames.map(childname => elem.name.concat(".").concat(childname)
          																																		.concat(" as ")
          																																		.concat(elem.name).concat("_").concat(childname))
					df=df.selectExpr(fieldNames:_*)
				case _ => //println("other type")
				}
				
			}
		}
		return df
	}



def flattenRecursive(df: DataFrame): DataFrame = {

    val fields = df.schema.fields
    val fieldNames = fields.map(x => x.name)
    val length = fields.length
    
    for(i <- 0 to fields.length-1){
      val field = fields(i)
      val fieldtype = field.dataType
      val fieldName = field.name
      fieldtype match {
        case arrayType: ArrayType => //println("flatten array")
          val newfieldNames = fieldNames.filter(_!=fieldName) ++ Array("explode_outer(".concat(fieldName).concat(") as ").concat(fieldName))
          val explodedDf = df.selectExpr(newfieldNames:_*)
          return flattenRecursive(explodedDf)
        case structType: StructType => //println("flatten struct")
          val newfieldNames = fieldNames.filter(_!= fieldName) ++ 
          										structType.fieldNames.map(childname => fieldName.concat(".").concat(childname)
          																																		.concat(" as ")
          																																		.concat(fieldName).concat("_").concat(childname))
         	val explodedf = df.selectExpr(newfieldNames:_*)
          return flattenRecursive(explodedf)
        case _ => //println("other type")
      }
    }
    df
  }
  
  
  //New method
  def flattenRecursive(inDf: DataFrame): DataFrame = {
    import org.apache.spark.storage.StorageLevel
    val df = inDf.repartition(200).persist(StorageLevel.MEMORY_AND_DISK)
    df.show(5, false)
    val fields = df.schema.fields
    val fieldNames = fields.map(x => x.name)
     for(i <- fields.indices){
      val field = fields(i)
      val fieldtype = field.dataType
      val fieldName = field.name
      fieldtype match {
        case _: ArrayType => //println("flatten array")
          val newfieldNames = fieldNames.filter(_!=fieldName) ++ Array("explode_outer(".concat(fieldName).concat(") as ").concat(fieldName))
          val explodedDf = df.selectExpr(newfieldNames:_*)
          return flattenRecursive(explodedDf)
        case structType: StructType => //println("flatten struct")
          val newfieldNames = fieldNames.filter(_!= fieldName) ++
            structType.fieldNames.map(childname => fieldName.concat(".").concat(childname)
              .concat(" as ")
              .concat(fieldName).concat("_").concat(childname))
          val explodedf = df.selectExpr(newfieldNames:_*)
          return flattenRecursive(explodedf)
        case _ => //println("other type")
      }
    }
    df
  }
